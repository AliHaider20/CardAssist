{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b311399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 161/161 [00:01<00:00, 130.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Standard Library\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "# Third-Party Libraries\n",
    "import faiss\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Langchain\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Semantic Kernel\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "\n",
    "# Event Loop Patch for Jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "search_api = os.getenv(\"AI_SEARCH_API\")\n",
    "search_endpoint = \"https://cardassist.search.windows.net\"\n",
    "ai_foundry_api = os.getenv(\"AI_FOUNDRY_MODEL_API\")\n",
    "llm_endpoint = os.getenv(\"LLM_ENDPOINT\")\n",
    "\n",
    "# Patch the event loop for Jupyter compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "pdf_doc_path = \"./global_card_access_user_guide.pdf\"\n",
    "loader = PyPDFLoader(pdf_doc_path)\n",
    "documents = loader.load()\n",
    "text_splitter = MarkdownTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "md_docs = text_splitter.split_documents(documents)\n",
    "md_docs = [doc.page_content for doc in md_docs]\n",
    "\n",
    "\n",
    "# Initialize embedding model\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embedding_service = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", trust_remote_code=True, device=device)\n",
    "\n",
    "# Generate and index embeddings\n",
    "embedding_matrix = np.array(\n",
    "    [embedding_service.encode([doc])[0] for doc in tqdm(md_docs, desc=\"Generating embeddings\")]\n",
    ").astype(\"float32\")\n",
    "\n",
    "index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
    "index.add(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c51fa",
   "metadata": {},
   "source": [
    "## Creating plugins for Activate, De-activate credit card and RAG QA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31350ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "class CreditCardPlugin:\n",
    "    @kernel_function(\n",
    "        description=\"Deactivate a credit card; returns a confirmation message\"\n",
    "    )\n",
    "    async def deactivate_card(self, card_number: Annotated[str, \"The credit card number to deactivate\"]) -> str:\n",
    "        print(\"Function called to deactivate card\")\n",
    "        return f\"Credit card {card_number} has been deactivated.\"\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Activate a credit card; returns a confirmation message\"\n",
    "    )\n",
    "    async def activate_card(self, card_number: Annotated[str, \"The credit card number to activate\"]) -> str:\n",
    "        print(\"Function called to activate card\")\n",
    "        return f\"Credit card {card_number} has been activated.\"\n",
    "    \n",
    "    @kernel_function(\n",
    "        description=\"Get card information and other general information about the card and account management\"\n",
    "    )\n",
    "    async def rag_query(self, query: Annotated[str, \"The user query for RAG (Retrieval-Augmented Generation)\"]):\n",
    "        print(\"Function called for RAG query\")\n",
    "        query_embedding = embedding_service.encode([query])\n",
    "        query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "\n",
    "        k = 10\n",
    "        distances, indices = index.search(query_embedding, k)\n",
    "        relevant_chunks = [md_docs[i] for i in indices[0]]\n",
    "\n",
    "        context = \"\\n\".join(relevant_chunks)\n",
    "        augmented_prompt = f\"{context}\\n\\nUser Query: {query}\"\n",
    "        \n",
    "        return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac11dac",
   "metadata": {},
   "source": [
    "## Initializing Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664196c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.core_plugins.time_plugin import TimePlugin\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "model_id = \"gpt-4o-mini\"\n",
    "\n",
    "# Add the Azure OpenAI chat completion service\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(deployment_name=model_id,\n",
    "                        endpoint=llm_endpoint,\n",
    "                        api_key=ai_foundry_api)\n",
    ")\n",
    "\n",
    "# Add a plugin\n",
    "kernel.add_plugin(\n",
    "    CreditCardPlugin(),\n",
    "    plugin_name=\"CreditCard\",\n",
    ")\n",
    "\n",
    "arguments = KernelArguments(\n",
    "    settings=AzureChatPromptExecutionSettings(\n",
    "        # Advertise all functions from the WeatherPlugin, DateTimePlugin, and LocationPlugin plugins to the AI model.\n",
    "        function_choice_behavior=FunctionChoiceBehavior.Auto(),\n",
    "        # function_choice_behavior=FunctionChoiceBehavior.Required(filters={\"included_functions\": [\"deactivate_card\", \"activate_card\"]}),\n",
    "        top_p= 0.9,\n",
    "        temperature=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "async def run_model(user_query: str , system_prompt: str ):\n",
    "    resp = await kernel.invoke_prompt(prompt = f\"{system_prompt}\\n\\nUser Query: {user_query}\", arguments=arguments )\n",
    "    return resp.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32899a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a helpful assistant that can answer questions about credit card management, including activating and deactivating cards, and providing information about card features and account management. Make sure you provide accurate and well structured information based on the provided context.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You're a helpful assistant that can answer questions about credit card management, including activating and deactivating cards, and providing information about card features and account management. Make sure you provide accurate and well structured information based on the provided context.\"\n",
    "\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2554c2ac",
   "metadata": {},
   "source": [
    "## Testing card activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a64ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called to activate card\n",
      "\n",
      "Response:\n",
      " Your credit card **1234-5678-9012-3456** has been successfully activated. If you have any other questions or need further assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Can you activate my credit card 1234-5678-9012-3456?\"\n",
    "\n",
    "# Run the asynchronous function\n",
    "response = asyncio.run(run_model(system_prompt=system_prompt, user_query=user_query))\n",
    "print(\"\\nResponse:\\n\", response[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7773d",
   "metadata": {},
   "source": [
    "## Testing card deactivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e29f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called to deactivate card\n",
      "\n",
      "Response:\n",
      " Your credit card ending in 3456 has been successfully deactivated. If you need any further assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Can you deactivate my credit card 1234-5678-9012-3456?\"\n",
    "\n",
    "# Run the asynchronous function\n",
    "response = asyncio.run(run_model(system_prompt=system_prompt, user_query=user_query))\n",
    "print(\"\\nResponse:\\n\", response[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf639bae",
   "metadata": {},
   "source": [
    "## Testing QA RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d1530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called for RAG query\n",
      "\n",
      "Response:\n",
      " To register as a new user for a corporate account, follow these steps:\n",
      "\n",
      "1. **Access the Global Card Access Website**: Go to [Global Card Access](https://www.bankofamerica.com/globalcardaccess).\n",
      "\n",
      "2. **Click on Register Now**: On the Global Card Access sign-in screen, click on the \"Register now\" option. \n",
      "   - If your organization has multiple corporate accounts, a \"Select Corporate Account(s)\" window will appear. Choose the appropriate corporate account and click OK.\n",
      "\n",
      "3. **Complete the Create Account Request Key(s) Page**: \n",
      "   - Configure your Account Request Key in the Settings section. This key will be used by your employees to request accounts online.\n",
      "\n",
      "4. **Add a Corporate Account**: \n",
      "   - If your company is set up for this feature, select \"Add\" from the bottom left-hand side of the Corporate Accounts screen to review and approve new account requests.\n",
      "\n",
      "5. **Creating Account Request Keys**: \n",
      "   - Each Account Request Key has specific settings that determine how an employee’s account is set up and managed, including the linked Corporate Account and the type of card the applicant will receive upon approval.\n",
      "\n",
      "6. **Assign Roles**: \n",
      "   - Click \"Add accounts\" to assign accounts that the user will have permissions over. Once you complete the role assignment, click \"Finish.\"\n",
      "\n",
      "7. **Receive Confirmation**: \n",
      "   - The user will receive a welcome email from Global Card Access containing their user ID and instructions for completing their registration.\n",
      "\n",
      "**Note**: The ability to set up Approval Groups and Approval Panels for reviewing and approving online account requests may vary based on your company’s configuration.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Steps for First-time Registration for Corporate Accounts\"\n",
    "\n",
    "# Run the asynchronous function\n",
    "response = asyncio.run(run_model(system_prompt=system_prompt, user_query=user_query))\n",
    "print(\"\\nResponse:\\n\", response[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a25d64",
   "metadata": {},
   "source": [
    "## Hybrid query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17220f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called to activate card\n",
      "Function called for RAG query\n",
      "\n",
      "Response:\n",
      " Your credit card **1234-5678-9012-3456** has been successfully activated.\n",
      "\n",
      "### Benefits and Features of the Card:\n",
      "1. **Global Card Access**: An online management tool that allows you to check your credit limit, balance, and available credit.\n",
      "2. **Security Features**: You can view and change your PIN, lock your card, and manage alerts for added security.\n",
      "3. **Convenient Payments**: Payments can be made online, providing ease of access and management.\n",
      "4. **Access to Statements**: You can view and download your statements for better financial tracking.\n",
      "5. **Customizable Alerts**: Set up alerts for due dates, spending limits, and other important notifications.\n",
      "\n",
      "If you have any more questions or need further assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Can you activate my card 1234-5678-9012-3456? Also, let me know it's benefits and features.\"\n",
    "\n",
    "# Run the asynchronous function\n",
    "response = asyncio.run(run_model(system_prompt=system_prompt, user_query=user_query))\n",
    "print(\"\\nResponse:\\n\", response[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d9a79",
   "metadata": {},
   "source": [
    "# Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81544039",
   "metadata": {},
   "source": [
    "## RAG QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf614fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "embed_endpoint = os.getenv(\"EMBEDDING_ENDPOINT\")\n",
    "\n",
    "model_name = \"text-embedding-3-large\"\n",
    "deployment = \"text-embedding-3-large\"\n",
    "\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "embed_client = OpenAITextEmbedding(   \n",
    "    # api_version=\"2024-12-01-preview\",\n",
    "    ai_model_id=\"gpt-4o-mini\",\n",
    "    api_key=ai_foundry_api,\n",
    "    # azure_endpoint=endpoint,\n",
    ")\n",
    "\n",
    "kernel.add_service(embed_client)\n",
    "\n",
    "# response = client.embeddings.create(\n",
    "#     input=[\"first phrase\",\"second phrase\",\"third phrase\"],\n",
    "#     model=deployment,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50877fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Annotated, List\n",
    "from uuid import uuid4\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding\n",
    "from semantic_kernel.connectors.memory.in_memory import InMemoryVectorStore\n",
    "from semantic_kernel.data import (\n",
    "    DistanceFunction,\n",
    "    IndexKind,\n",
    "    VectorStoreRecordDataField,\n",
    "    VectorStoreRecordKeyField,\n",
    "    VectorStoreRecordVectorField,\n",
    "    vectorstoremodel,\n",
    ")\n",
    "\n",
    "# Define the data model\n",
    "@vectorstoremodel\n",
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    content: Annotated[str, VectorStoreRecordDataField(is_full_text_searchable=True)]\n",
    "    embedding: Annotated[List[float], VectorStoreRecordVectorField(\n",
    "        dimensions=1536,  # Adjust based on your embedding model's output\n",
    "        distance_function=DistanceFunction.COSINE_SIMILARITY,\n",
    "        index_kind=IndexKind.FLAT\n",
    "    )]\n",
    "    id: Annotated[str, VectorStoreRecordKeyField()] = field(default_factory=lambda: str(uuid4()))\n",
    "\n",
    "async def main():\n",
    "    # Initialize the kernel\n",
    "    kernel = Kernel()\n",
    "\n",
    "    # Initialize the embedding model\n",
    "    # embedding_model = OpenAITextEmbedding(service_id=\"embedder\")  # Ensure this matches your configuration\n",
    "    kernel.add_service(embed_client)\n",
    "\n",
    "    # Initialize the in-memory vector store\n",
    "    vector_store = InMemoryVectorStore()\n",
    "    kernel.add_service(vector_store)\n",
    "\n",
    "    # Get or create the collection\n",
    "    collection = vector_store.get_collection(\"pdf_chunks\", data_model_type=DocumentChunk)\n",
    "    await collection.create_collection_if_not_exists()\n",
    "\n",
    "\n",
    "    # Process and upsert each document chunk\n",
    "    for chunk in md_docs:\n",
    "        embedding = await embedding_model.generate_embeddings([chunk])\n",
    "        record = DocumentChunk(content=chunk, embedding=embedding[0])\n",
    "        await collection.upsert(record)\n",
    "\n",
    "# Run the asynchronous main function\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search_similar_documents(query: str):\n",
    "    # Generate embedding for the query\n",
    "    response = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        engine=\"text-embedding-ada-002\"  # Replace with your deployed model name\n",
    "    )\n",
    "    query_embedding = response['data'][0]['embedding']\n",
    "\n",
    "    # Perform vector search\n",
    "    search_results = await collection.vectorized_search(vector=query_embedding, top=5)\n",
    "\n",
    "    # Display the results\n",
    "    for result in search_results.results:\n",
    "        print(f\"Score: {result.score:.4f}, Content: {result.record.content}\")\n",
    "\n",
    "# Example usage\n",
    "asyncio.run(search_similar_documents(\"What are the steps to deactivate a credit card?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb95b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.core_plugins.time_plugin import TimePlugin\n",
    "from semantic_kernel.prompt_template import KernelPromptTemplate\n",
    "from semantic_kernel.prompt_template.prompt_template_config import PromptTemplateConfig\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Configure Azure OpenAI service\n",
    "model_id = \"gpt-4o-mini\"\n",
    "\n",
    "kernel.add_service(AzureChatCompletion(deployment_name=model_id, endpoint=llm_endpoint, api_key=ai_foundry_api))\n",
    "\n",
    "# Add plugins if necessary\n",
    "kernel.add_plugin(TimePlugin(), plugin_name=\"Time\")\n",
    "\n",
    "async def answer_query(query: str):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True).tolist()[0]\n",
    "    # Perform vector search\n",
    "    search_results = await collection.vectorized_search(\n",
    "    vector=query_embedding,\n",
    "    vector_name=\"text-dense\",\n",
    "    top=5\n",
    "    )\n",
    "    # Compile context from search results\n",
    "    context = \"\\n\".join([result.record.content for result in search_results.results])\n",
    "    # Define the prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are an assistant that provides answers based on the following context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    Answer:\"\"\"\n",
    "    # Invoke the model\n",
    "    response = await kernel.invoke_prompt(prompt)\n",
    "    return response.value\n",
    "\n",
    "\n",
    "response = await answer_query(\"What are the steps to deactivate a credit card?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from semantic_kernel.data import (\n",
    "    DistanceFunction,\n",
    "    IndexKind,\n",
    "    VectorStoreRecordVectorField,\n",
    "    vectorstoremodel,\n",
    ")\n",
    "\n",
    "@vectorstoremodel\n",
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    content: Annotated[str, VectorStoreRecordDataField(is_full_text_searchable=True)]\n",
    "    embedding: Annotated[List[float], VectorStoreRecordVectorField(dimensions=512, distance_function=DistanceFunction.COSINE_SIMILARITY, index_kind=IndexKind.HNSW)]\n",
    "    id: Annotated[str, VectorStoreRecordKeyField()] = field(default_factory=lambda: str(uuid4()))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1039e6",
   "metadata": {},
   "source": [
    "## Different approach\n",
    "\n",
    "https://medium.com/@j.wang.mlds_97641/building-a-rag-pipeline-with-semantic-kernel-a-step-by-step-guide-7e7e3617a62b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51843fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextEmbedding\n",
    "from semantic_kernel import Kernel\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add the Hugging Face embedding service\n",
    "kernel.add_service(\n",
    "    HuggingFaceTextEmbedding(\n",
    "        ai_model_id=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0281590",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.add_embedding_to_object([\"hello\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e37de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import OpenAITextCompletion, AzureTextEmbedding\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "from azure.ai.inference import EmbeddingsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "\n",
    "kernel = Kernel()\n",
    "# Add text completion service\n",
    "# kernel.add_service(\n",
    "#     service=OpenAITextCompletion(\n",
    "#         ai_model_id=\"gpt-4o-mini\",\n",
    "#         api_key=ai_foundry_api\n",
    "#     )\n",
    "# )\n",
    "# Add embedding generation service\n",
    "# kernel.add_service(\n",
    "#     service=AzureTextEmbedding(\n",
    "#         deployment_name=\"text-embedding-3-large\",\n",
    "#         base_url=embed_endpoint,\n",
    "#         api_key=ai_foundry_api\n",
    "#     )\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "model_name = \"text-embedding-3-small\"\n",
    "deployment = \"text-embedding-3-small\"\n",
    "\n",
    "api_version = \"2024-02-01\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    # endpoint=endpoint,\n",
    "    api_key=ai_foundry_api,\n",
    "    azure_endpoint=embed_endpoint\n",
    ")\n",
    "\n",
    "client.service_id = \"embedder\"\n",
    "\n",
    "kernel.add_service(client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9395a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.ai.inference import EmbeddingsClient\n",
    "\n",
    "model_name = \"text-embedding-3-small\"\n",
    "\n",
    "client = EmbeddingsClient(\n",
    "    endpoint=embed_endpoint,\n",
    "    credential=AzureKeyCredential(ai_foundry_api),\n",
    ")\n",
    "\n",
    "kernel.add_service(service=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c93d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.memory import memory_store_base\n",
    "from semantic_kernel.memory.memory_record import MemoryRecord\n",
    "from semantic_kernel.connectors.memory.qdrant import QdrantMemoryStore\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\n",
    "\n",
    "memory_store = SemanticTextMemory(embeddings_generator=kernel.services.get(\"sentence-transformers/all-MiniLM-L6-v2\"), storage=QdrantMemoryStore(vector_size=512))\n",
    "kernel.add_plugin(memory_store)\n",
    "\n",
    "# Sample knowledge base\n",
    "documents = [\n",
    "    (\"doc1\", \"Semantic Kernel enables lightweight orchestration of LLMs and skills.\"),\n",
    "    (\"doc2\", \"RAG pipelines combine retrieval with generative models for improved responses.\")\n",
    "]\n",
    "\n",
    "for doc_id, content in documents:\n",
    "    memory_store.save_information(\n",
    "        collection=\"knowledge-base\",\n",
    "        text=content,\n",
    "        id=doc_id,\n",
    "        # =MemoryRecord.local_record(id=doc_id, text=content, description=\"\", additional_metadata=\"\", embedding=embed_client.embeddings.create(input=[content], model=deployment).data[0].embedding)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27022430",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdocs = await memory_store.search(collection=\"knowledge-base\", query=\"What is Semantic Kernel?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant. Answer the following question using the context below.\n",
    "\n",
    "Context:\n",
    "{{$retrieved_context}}\n",
    "\n",
    "Question: {{$input}}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "rag_function = kernel.create_semantic_function(\n",
    "    prompt_template,\n",
    "    description=\"RAG Answer Generator\",\n",
    "    max_tokens=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "await rag_function.send(\"Helo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf51dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e058f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.prompt_execution_settings import PromptExecutionSettings\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Hugging Face embedding service\n",
    "embedding_service = HuggingFaceTextEmbedding(ai_model_id=\"sentence-transformers/all-MiniLM-L6-v2\", service_id=\"embedder\")\n",
    "kernel.add_service(embedding_service)\n",
    "\n",
    "# Add Azure OpenAI chat completion service\n",
    "# Replace with your actual deployment name, endpoint, and API key\n",
    "chat_service = AzureChatCompletion(\n",
    "    deployment_name=\"your-deployment-name\",\n",
    "    api_key=\"your-api-key\"\n",
    ")\n",
    "kernel.add_service(chat_service)\n",
    "\n",
    "# Load and split the PDF document\n",
    "pdf_doc_path = \"global_card_access_user_guide.pdf\"\n",
    "loader = PyPDFLoader(pdf_doc_path)\n",
    "documents = loader.load()\n",
    "text_splitter = MarkdownTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "md_docs = text_splitter.split_documents(documents)\n",
    "md_docs = [doc.page_content for doc in md_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b0ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.get_service(\"embedder\").encode(\n",
    "    texts=md_docs,\n",
    "    model=\"text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9931e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Assuming embedding_service is an instance of HuggingFaceTextEmbedding\n",
    "async def get_embedding():\n",
    "    embed = await embedding_service.generate_embeddings([\"hello\"])\n",
    "    return embed\n",
    "\n",
    "# Run the asynchronous function\n",
    "embedding = asyncio.run(get_embedding())\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0a5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "\n",
    "# Allow nested event loops for compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Required credentials\n",
    "llm_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "ai_foundry_api = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "\n",
    "# Initialize Semantic Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Hugging Face embedding service\n",
    "embedding_service = HuggingFaceTextEmbedding(ai_model_id=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "kernel.add_service(embedding_service)\n",
    "\n",
    "# Add Azure OpenAI Chat Completion\n",
    "chat_service = AzureChatCompletion(\n",
    "    deployment_name=\"gpt-4o-mini\",\n",
    "    endpoint=llm_endpoint,\n",
    "    base_url=llm_endpoint,\n",
    "    api_key=ai_foundry_api\n",
    ")\n",
    "kernel.add_service(chat_service)\n",
    "\n",
    "# Load and chunk the document\n",
    "pdf_doc_path = \"/Users/ha/Desktop/Projects/Wipro task/global_card_access_user_guide.pdf\"\n",
    "loader = PyPDFLoader(pdf_doc_path)\n",
    "documents = loader.load()\n",
    "text_splitter = MarkdownTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "md_docs = text_splitter.split_documents(documents)\n",
    "md_docs = [doc.page_content for doc in md_docs]\n",
    "\n",
    "# Initialize FAISS index (before inserting embeddings)\n",
    "embedding_dim = 384  # For 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Async function to generate and add embeddings one by one\n",
    "import asyncio\n",
    "\n",
    "async def generate_and_add_embeddings(docs):\n",
    "    for doc in tqdm(docs):\n",
    "        embedding = await embedding_service.generate_embeddings([doc])\n",
    "        embedding_np = np.array(embedding).astype(\"float32\")\n",
    "        index.add(embedding_np)\n",
    "\n",
    "\n",
    "# Run the async embedding generation and insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb93d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# await generate_and_add_embeddings(md_docs)\n",
    "\n",
    "async def generate_all_embeddings(doc):\n",
    "    embedding = asyncio.run(embedding_service.generate_embeddings([doc]))\n",
    "    return embedding\n",
    "    \n",
    "await asyncio.run(generate_all_embeddings(md_docs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a03d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.hugging_face import HuggingFaceTextEmbedding\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_service = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# Allow nested event loops for compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Required credentials\n",
    "llm_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "ai_foundry_api = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "\n",
    "# Initialize Semantic Kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Hugging Face embedding service\n",
    "# embedding_service = HuggingFaceTextEmbedding(ai_model_id=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# kernel.add_service(embedding_service)\n",
    "\n",
    "# Add Azure OpenAI Chat Completion\n",
    "chat_service = AzureChatCompletion(\n",
    "    deployment_name=\"gpt-4o-mini\",\n",
    "    endpoint=llm_endpoint,\n",
    "    base_url=llm_endpoint,\n",
    "    api_key=ai_foundry_api\n",
    ")\n",
    "kernel.add_service(chat_service)\n",
    "\n",
    "# Load and chunk the document\n",
    "pdf_doc_path = \"/Users/ha/Desktop/Projects/Wipro task/global_card_access_user_guide.pdf\"\n",
    "loader = PyPDFLoader(pdf_doc_path)\n",
    "documents = loader.load()\n",
    "text_splitter = MarkdownTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "md_docs = text_splitter.split_documents(documents)\n",
    "md_docs = [doc.page_content for doc in md_docs]\n",
    "\n",
    "# Generate document embeddings\n",
    "def generate_all_embeddings(docs):\n",
    "    embeddings = []\n",
    "    for doc in docs:\n",
    "        embedding = embedding_service.encode([doc])\n",
    "        embeddings.append(embedding[0])\n",
    "    return embeddings\n",
    "\n",
    "# Embed and index\n",
    "embedding_matrix = np.array(generate_all_embeddings(md_docs)).astype(\"float32\")\n",
    "index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
    "index.add(embedding_matrix)\n",
    "\n",
    "# RAG Query Function\n",
    "async def rag_query(query: str):\n",
    "    query_embedding = embedding_service.encode([query])\n",
    "    query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "\n",
    "    k = 10\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    relevant_chunks = [md_docs[i] for i in indices[0]]\n",
    "\n",
    "    context = \"\\n\".join(relevant_chunks)\n",
    "    augmented_prompt = f\"{context}\\n\\nUser Query: {query}\"\n",
    "\n",
    "    arguments = KernelArguments(\n",
    "        settings=AzureChatPromptExecutionSettings(\n",
    "            function_choice_behavior=FunctionChoiceBehavior.Auto(),\n",
    "            top_p=0.9,\n",
    "            temperature=0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    response = await kernel.invoke_prompt(augmented_prompt, arguments=arguments)\n",
    "    return response.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfee780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST THE PIPELINE\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"How do I activate my card?\"\n",
    "    output = asyncio.run(rag_query(user_query))\n",
    "    print(\"\\nResponse:\\n\", output[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1787d750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
